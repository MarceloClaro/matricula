"""
M√≥dulo de Reconhecimento Facial com Anti-Spoofing
Implementa captura de sequ√™ncia de fotos, treinamento de modelo e detec√ß√£o de faces
"""
import numpy as np
import os
import pickle
import json
import time
from datetime import datetime
from PIL import Image

# Tentar importar cv2 (opencv)
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

# Tentar importar face_recognition e bibliotecas opcionais
try:
    import face_recognition
    FACE_RECOGNITION_AVAILABLE = True
except ImportError:
    FACE_RECOGNITION_AVAILABLE = False

try:
    import imgaug.augmenters as iaa
    IMGAUG_AVAILABLE = True
except ImportError:
    IMGAUG_AVAILABLE = False

# TensorFlow and scikit-learn for anti-spoofing
TENSORFLOW_AVAILABLE = False
SKLEARN_AVAILABLE = False
try:
    from sklearn.model_selection import train_test_split
    SKLEARN_AVAILABLE = True
except ImportError:
    pass

try:
    from tensorflow import keras
    from tensorflow.keras import layers, callbacks
    from tensorflow.keras.models import Sequential, load_model
    TENSORFLOW_AVAILABLE = True
except ImportError:
    pass

# Import streamlit after optional imports to avoid import-time warnings
import streamlit as st

class FaceRecognitionSystem:
    """Sistema de reconhecimento facial com anti-spoofing"""
    
    # Constantes de qualidade de imagem
    MIN_SHARPNESS = 50
    IDEAL_BRIGHTNESS = 128
    MIN_FACE_SIZE_RATIO = 0.2
    MAX_FACE_SIZE_RATIO = 0.4
    
    # Pesos para score de qualidade
    SHARPNESS_WEIGHT = 0.35
    BRIGHTNESS_WEIGHT = 0.25
    FACE_SIZE_WEIGHT = 0.40
    
    # Limites de valida√ß√£o de treinamento
    EXCELLENT_DISTANCE = 0.4
    GOOD_DISTANCE = 0.6
    ACCEPTABLE_DISTANCE = 0.7
    
    # Thresholds adaptativos de reconhecimento
    THRESHOLD_DEFAULT = 0.50
    THRESHOLD_RELAXED = 0.55
    THRESHOLD_STRICT = 0.45
    THRESHOLD_DIFF_MIN = 0.1  # Diferen√ßa m√≠nima entre 1¬∫ e 2¬∫ para usar threshold relaxado
    
    def __init__(self, data_dir='data'):
        self.data_dir = data_dir
        self.faces_dir = os.path.join(data_dir, 'faces')
        self.models_dir = os.path.join(data_dir, 'models')
        # Sistema est√° dispon√≠vel apenas se todas as depend√™ncias est√£o instaladas
        self.available = FACE_RECOGNITION_AVAILABLE and CV2_AVAILABLE
        
        # Criar diret√≥rios se n√£o existirem
        os.makedirs(self.faces_dir, exist_ok=True)
        os.makedirs(self.models_dir, exist_ok=True)
        
        # Caminhos dos modelos
        self.embeddings_path = os.path.join(self.models_dir, 'face_embeddings.pkl')
        self.liveness_model_path = os.path.join(self.models_dir, 'liveness_model.h5')
        
        # Carregar embeddings se existirem
        self.known_face_encodings = []
        self.known_face_ids = []
        if self.available:
            self.load_embeddings()
        
        # Carregar modelo de liveness se existir
        self.liveness_model = None
        if TENSORFLOW_AVAILABLE and os.path.exists(self.liveness_model_path):
            try:
                self.liveness_model = load_model(self.liveness_model_path)
            except (OSError, ValueError) as e:
                # Log error but continue without liveness model
                self.liveness_model = None
    
    def assess_image_quality(self, frame):
        """
        Avalia a qualidade de uma imagem para reconhecimento facial
        
        Args:
            frame: Frame capturado (numpy array)
        
        Returns:
            dict: M√©tricas de qualidade (score, brightness, sharpness, has_face)
        """
        if not CV2_AVAILABLE:
            return {'score': 0, 'brightness': 0, 'sharpness': 0, 'has_face': False}
        
        # Converter para escala de cinza
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # 1. Avaliar nitidez (Laplacian variance)
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        sharpness_score = min(laplacian_var / 100.0, 1.0)  # Normalizar para 0-1
        
        # 2. Avaliar brilho (m√©dia de intensidade)
        brightness = gray.mean()
        brightness_score = 1.0 - abs(brightness - self.IDEAL_BRIGHTNESS) / self.IDEAL_BRIGHTNESS
        
        # 3. Detectar face
        has_face = False
        face_size_score = 0.0
        face_locations = None
        
        if self.available:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            face_locations = face_recognition.face_locations(rgb_frame, model='hog')
            
            if len(face_locations) > 0:
                has_face = True
                # Avaliar tamanho da face (face maior = melhor)
                top, right, bottom, left = face_locations[0]
                face_height = bottom - top
                frame_height, frame_width = frame.shape[:2]
                
                # Face ideal ocupa MIN_FACE_SIZE_RATIO-MAX_FACE_SIZE_RATIO da altura do frame
                face_ratio = face_height / frame_height
                if self.MIN_FACE_SIZE_RATIO <= face_ratio <= self.MAX_FACE_SIZE_RATIO:
                    face_size_score = 1.0
                elif face_ratio < self.MIN_FACE_SIZE_RATIO:
                    face_size_score = face_ratio / self.MIN_FACE_SIZE_RATIO
                else:
                    face_size_score = self.MAX_FACE_SIZE_RATIO / face_ratio
        
        # Calcular score geral (ponderado)
        if not has_face:
            overall_score = 0.0
        else:
            overall_score = (
                sharpness_score * self.SHARPNESS_WEIGHT +
                brightness_score * self.BRIGHTNESS_WEIGHT +
                face_size_score * self.FACE_SIZE_WEIGHT
            )
        
        return {
            'score': overall_score,
            'brightness': brightness,
            'sharpness': laplacian_var,
            'has_face': has_face,
            'face_size_score': face_size_score
        }
    
    def capture_photo_sequence(self, aluno_id, num_photos=30, duration=10, quality_threshold=0.5):
        """
        Captura uma sequ√™ncia de fotos usando a webcam com valida√ß√£o de qualidade
        
        Args:
            aluno_id: ID do aluno
            num_photos: N√∫mero de fotos a capturar (padr√£o: 30)
            duration: Dura√ß√£o em segundos (padr√£o: 10)
            quality_threshold: Limiar m√≠nimo de qualidade (padr√£o: 0.5)
        
        Returns:
            list: Lista de caminhos das fotos salvas
        """
        if not CV2_AVAILABLE:
            st.error("‚ùå OpenCV (cv2) n√£o est√° dispon√≠vel. Instale opencv-python ou opencv-python-headless.")
            return []
        
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            st.error("N√£o foi poss√≠vel acessar a webcam")
            return []
        
        # Criar diret√≥rio para o aluno
        aluno_dir = os.path.join(self.faces_dir, f'aluno_{aluno_id}')
        os.makedirs(aluno_dir, exist_ok=True)
        
        photos_saved = []
        quality_scores = []
        interval = duration / num_photos  # Intervalo entre fotos
        
        st.info(f"""
        üé• **Captura Inteligente de Fotos**
        - Alvo: {num_photos} fotos de alta qualidade
        - Dura√ß√£o: {duration} segundos
        - Qualidade m√≠nima: {quality_threshold*100:.0f}%
        
        üí° **Dicas para melhor qualidade:**
        - Mantenha o rosto centralizado
        - Ilumina√ß√£o uniforme no rosto
        - Evite movimentos bruscos
        """)
        
        progress_bar = st.progress(0)
        placeholder = st.empty()
        quality_placeholder = st.empty()
        
        start_time = datetime.now()
        photo_count = 0
        attempts = 0
        max_attempts = min(num_photos * 3, 150)  # Limitar para evitar loops infinitos
        
        while photo_count < num_photos and attempts < max_attempts:
            ret, frame = cap.read()
            if not ret:
                break
            
            attempts += 1
            elapsed = (datetime.now() - start_time).total_seconds()
            
            # Avaliar qualidade do frame atual
            quality = self.assess_image_quality(frame)
            
            # Mostrar feedback em tempo real
            frame_display = frame.copy()
            frame_rgb = cv2.cvtColor(frame_display, cv2.COLOR_BGR2RGB)
            
            # Adicionar indicadores visuais
            if quality['has_face']:
                color = (0, 255, 0) if quality['score'] >= quality_threshold else (255, 165, 0)
                cv2.putText(frame_display, f"Qualidade: {quality['score']:.2f}", 
                          (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
                
                # Detectar e desenhar ret√¢ngulo na face
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                face_locations = face_recognition.face_locations(rgb_frame, model='hog')
                if len(face_locations) > 0:
                    top, right, bottom, left = face_locations[0]
                    cv2.rectangle(frame_display, (left, top), (right, bottom), color, 2)
            else:
                cv2.putText(frame_display, "Nenhuma face detectada", 
                          (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            
            frame_rgb = cv2.cvtColor(frame_display, cv2.COLOR_BGR2RGB)
            
            # Capturar foto se qualidade for boa e tempo adequado
            if (elapsed >= photo_count * interval and 
                quality['score'] >= quality_threshold and 
                quality['has_face']):
                
                # Salvar foto
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')
                photo_path = os.path.join(aluno_dir, f'photo_{timestamp}.jpg')
                cv2.imwrite(photo_path, frame)
                photos_saved.append(photo_path)
                quality_scores.append(quality['score'])
                photo_count += 1
                
                # Atualizar progresso
                progress_bar.progress(photo_count / num_photos)
                
                # Mostrar frame capturado com indicador de sucesso
                cv2.putText(frame_display, "CAPTURADA!", 
                          (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                frame_rgb = cv2.cvtColor(frame_display, cv2.COLOR_BGR2RGB)
            
            # Atualizar visualiza√ß√£o
            placeholder.image(frame_rgb, caption=f'Foto {photo_count}/{num_photos} | Qualidade: {quality["score"]:.2%}', 
                            use_column_width=True)
            
            # Mostrar m√©tricas de qualidade
            quality_placeholder.info(f"""
            üìä **M√©tricas em Tempo Real:**
            - Face detectada: {'‚úÖ Sim' if quality['has_face'] else '‚ùå N√£o'}
            - Brilho: {quality['brightness']:.0f}/255 (ideal: ~128)
            - Nitidez: {quality['sharpness']:.0f} (m√≠nimo: ~50)
            - Score geral: {quality['score']:.2%}
            """)
            
            if elapsed >= duration:
                break
        
        cap.release()
        progress_bar.empty()
        placeholder.empty()
        quality_placeholder.empty()
        
        # Mostrar resumo da captura
        if len(photos_saved) > 0:
            avg_quality = sum(quality_scores) / len(quality_scores)
            st.success(f"""
            ‚úÖ **Captura conclu√≠da com sucesso!**
            
            - Fotos capturadas: {len(photos_saved)}
            - Qualidade m√©dia: {avg_quality:.2%}
            - Qualidade m√≠nima: {min(quality_scores):.2%}
            - Qualidade m√°xima: {max(quality_scores):.2%}
            """)
        else:
            st.error("‚ùå Nenhuma foto de qualidade suficiente foi capturada. Tente novamente com melhor ilumina√ß√£o.")
        
        return photos_saved
    
    def augment_images(self, image_paths):
        """
        Aplica data augmentation nas imagens
        
        Args:
            image_paths: Lista de caminhos das imagens
        
        Returns:
            list: Lista de imagens aumentadas (numpy arrays)
        """
        if not CV2_AVAILABLE:
            # Sem cv2, n√£o podemos processar imagens
            return []
        
        if not IMGAUG_AVAILABLE:
            # Sem augmentation, retornar apenas as imagens originais
            images = []
            for img_path in image_paths:
                image = cv2.imread(img_path)
                if image is not None:
                    images.append(image)
            return images
        
        # Definir augmentations
        seq = iaa.Sequential([
            iaa.Fliplr(0.5),  # Flip horizontal em 50% das imagens
            iaa.Affine(
                rotate=(-10, 10),  # Rota√ß√£o de -10 a 10 graus
                scale=(0.9, 1.1),  # Escala de 90% a 110%
            ),
            iaa.Multiply((0.8, 1.2)),  # Mudar brilho
            iaa.GaussianBlur(sigma=(0, 0.5)),  # Blur gaussiano leve
        ])
        
        augmented_images = []
        for img_path in image_paths:
            # Carregar imagem
            image = cv2.imread(img_path)
            if image is None:
                continue
            
            # Imagem original
            augmented_images.append(image)
            
            # Aplicar augmentations (2 varia√ß√µes por imagem)
            for _ in range(2):
                aug_image = seq(image=image)
                augmented_images.append(aug_image)
        
        return augmented_images
    
    def extract_face_encodings(self, image_paths, aluno_id):
        """
        Extrai encodings das faces das imagens
        
        Args:
            image_paths: Lista de caminhos das imagens
            aluno_id: ID do aluno
        
        Returns:
            list: Lista de encodings extra√≠dos
        """
        if not self.available:
            st.error("‚ùå Reconhecimento facial n√£o est√° dispon√≠vel. Instale face_recognition e dlib.")
            return []
        
        encodings = []
        
        # Aplicar augmentation
        augmented_images = self.augment_images(image_paths)
        
        progress_bar = st.progress(0)
        st.info(f"Processando {len(augmented_images)} imagens (incluindo augmentation)...")
        
        for idx, image in enumerate(augmented_images):
            # Converter para RGB
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # Detectar faces
            face_locations = face_recognition.face_locations(rgb_image, model='hog')
            
            if len(face_locations) > 0:
                # Extrair encoding da primeira face detectada
                face_encodings = face_recognition.face_encodings(rgb_image, face_locations)
                if len(face_encodings) > 0:
                    encodings.append(face_encodings[0])
            
            progress_bar.progress((idx + 1) / len(augmented_images))
        
        progress_bar.empty()
        st.success(f"‚úÖ {len(encodings)} encodings extra√≠dos com sucesso!")
        
        return encodings
    
    def validate_training_quality(self, encodings, aluno_id):
        """
        Valida a qualidade do treinamento verificando consist√™ncia dos encodings
        
        Args:
            encodings: Lista de encodings para validar
            aluno_id: ID do aluno
        
        Returns:
            dict: M√©tricas de qualidade (consistency_score, avg_distance, is_valid)
        """
        if len(encodings) < 2:
            return {
                'consistency_score': 0.0,
                'avg_distance': 0.0,
                'is_valid': len(encodings) > 0
            }
        
        # Otimiza√ß√£o: Para muitos encodings, amostrar para evitar O(n¬≤)
        max_sample_size = 50
        if len(encodings) > max_sample_size:
            import random
            sample_indices = random.sample(range(len(encodings)), max_sample_size)
            sampled_encodings = [encodings[i] for i in sample_indices]
        else:
            sampled_encodings = encodings
        
        # Calcular dist√¢ncia m√©dia entre todos os pares de encodings
        distances = []
        for i in range(len(sampled_encodings)):
            # Calcular dist√¢ncias vetorizadas para este encoding
            other_encodings = sampled_encodings[i+1:]
            if other_encodings:
                dists = face_recognition.face_distance(other_encodings, sampled_encodings[i])
                distances.extend(dists)
        
        avg_distance = sum(distances) / len(distances) if distances else 0.0
        
        # Score de consist√™ncia (menor dist√¢ncia = maior consist√™ncia)
        consistency_score = 1.0 - min(avg_distance / self.GOOD_DISTANCE, 1.0)
        
        # Considerar v√°lido se consist√™ncia for razo√°vel
        is_valid = avg_distance < self.ACCEPTABLE_DISTANCE
        
        return {
            'consistency_score': consistency_score,
            'avg_distance': avg_distance,
            'is_valid': is_valid,
            'num_encodings': len(encodings)
        }
    
    def train_face_recognition(self, aluno_id, image_paths):
        """
        Treina o modelo de reconhecimento facial com as imagens do aluno
        Inclui valida√ß√£o de qualidade e m√©tricas detalhadas
        
        Args:
            aluno_id: ID do aluno
            image_paths: Lista de caminhos das imagens
        
        Returns:
            bool: True se treinamento foi bem sucedido
        """
        if not self.available:
            st.error("‚ùå Reconhecimento facial n√£o est√° dispon√≠vel. Instale face_recognition e dlib.")
            return False
        
        # Extrair encodings
        encodings = self.extract_face_encodings(image_paths, aluno_id)
        
        if len(encodings) == 0:
            st.error("‚ùå Nenhuma face detectada nas imagens!")
            return False
        
        # Validar qualidade do treinamento
        st.info("üîç Validando qualidade do treinamento...")
        validation = self.validate_training_quality(encodings, aluno_id)
        
        if not validation['is_valid']:
            st.warning(f"""
            ‚ö†Ô∏è **Qualidade do treinamento abaixo do ideal**
            
            A consist√™ncia entre as imagens est√° baixa. Isso pode ocorrer se:
            - A ilumina√ß√£o variou muito durante a captura
            - Houve muitos movimentos ou mudan√ßas de express√£o
            - A qualidade das imagens foi inconsistente
            
            **Recomenda√ß√£o:** Considere recapturar as fotos com:
            - Ilumina√ß√£o mais uniforme
            - Menos movimentos bruscos
            - Posi√ß√£o mais centralizada
            
            O sistema ainda funcionar√°, mas pode ter precis√£o reduzida.
            """)
        
        # Adicionar aos encodings conhecidos
        self.known_face_encodings.extend(encodings)
        self.known_face_ids.extend([aluno_id] * len(encodings))
        
        # Salvar embeddings
        self.save_embeddings()
        
        # Mostrar m√©tricas detalhadas
        quality_label = (
            '‚≠ê Excelente' if validation['avg_distance'] < self.EXCELLENT_DISTANCE 
            else '‚úÖ Boa' if validation['avg_distance'] < self.GOOD_DISTANCE 
            else '‚ö†Ô∏è Aceit√°vel'
        )
        
        st.success(f"""
        ‚úÖ **Treinamento conclu√≠do com sucesso!**
        
        üìä **M√©tricas do Modelo:**
        - Encodings gerados: {len(encodings)}
        - Consist√™ncia: {validation['consistency_score']:.2%}
        - Dist√¢ncia m√©dia interna: {validation['avg_distance']:.3f}
        - Qualidade: {quality_label}
        
        üí° **Interpreta√ß√£o:**
        - Dist√¢ncia < {self.EXCELLENT_DISTANCE}: Excelente qualidade
        - Dist√¢ncia {self.EXCELLENT_DISTANCE}-{self.GOOD_DISTANCE}: Boa qualidade (recomendado)
        - Dist√¢ncia {self.GOOD_DISTANCE}-{self.ACCEPTABLE_DISTANCE}: Aceit√°vel
        - Dist√¢ncia > {self.ACCEPTABLE_DISTANCE}: Considere retreinar
        """)
        
        return True
    
    def save_embeddings(self):
        """Salva os embeddings em arquivo"""
        data = {
            'encodings': self.known_face_encodings,
            'ids': self.known_face_ids
        }
        with open(self.embeddings_path, 'wb') as f:
            pickle.dump(data, f)
    
    def load_embeddings(self):
        """Carrega os embeddings do arquivo"""
        if os.path.exists(self.embeddings_path):
            try:
                with open(self.embeddings_path, 'rb') as f:
                    data = pickle.load(f)
                    self.known_face_encodings = data['encodings']
                    self.known_face_ids = data['ids']
            except (EOFError, pickle.UnpicklingError, KeyError) as e:
                # If embeddings file is corrupted, start fresh
                self.known_face_encodings = []
                self.known_face_ids = []
    
    def recognize_face(self, frame, return_rankings=False, adaptive_threshold=True):
        """
        Reconhece faces em um frame com ranking de candidatos
        
        Args:
            frame: Frame capturado da webcam (numpy array)
            return_rankings: Se True, retorna top 3 candidatos
            adaptive_threshold: Se True, usa threshold adaptativo
        
        Returns:
            Se return_rankings=False: tuple (aluno_id, confidence, face_location) ou (None, 0, None)
            Se return_rankings=True: tuple (aluno_id, confidence, face_location, rankings)
        """
        if not self.available:
            return (None, 0, None, []) if return_rankings else (None, 0, None)
        
        if len(self.known_face_encodings) == 0:
            return (None, 0, None, []) if return_rankings else (None, 0, None)
        
        # Converter para RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # Detectar faces
        face_locations = face_recognition.face_locations(rgb_frame, model='hog')
        
        if len(face_locations) == 0:
            return (None, 0, None, []) if return_rankings else (None, 0, None)
        
        # Extrair encodings
        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
        
        for face_encoding, face_location in zip(face_encodings, face_locations):
            # Calcular dist√¢ncias para todas as faces conhecidas
            face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)
            
            if len(face_distances) == 0:
                return (None, 0, None, []) if return_rankings else (None, 0, None)
            
            # Agrupar por aluno_id e calcular dist√¢ncia m√©dia
            aluno_distances = {}
            for idx, (aluno_id, distance) in enumerate(zip(self.known_face_ids, face_distances)):
                if aluno_id not in aluno_distances:
                    aluno_distances[aluno_id] = []
                aluno_distances[aluno_id].append(distance)
            
            # Calcular m√©dia de dist√¢ncias por aluno
            aluno_avg_distances = {
                aluno_id: sum(distances) / len(distances)
                for aluno_id, distances in aluno_distances.items()
            }
            
            # Ordenar por menor dist√¢ncia
            sorted_alunos = sorted(aluno_avg_distances.items(), key=lambda x: x[1])
            
            # Determinar threshold
            if adaptive_threshold and len(sorted_alunos) > 0:
                # Threshold adaptativo: se h√° diferen√ßa significativa entre primeiro e segundo
                best_distance = sorted_alunos[0][1]
                if len(sorted_alunos) > 1:
                    second_distance = sorted_alunos[1][1]
                    # Se a diferen√ßa √© grande, podemos ser mais confiantes
                    if (second_distance - best_distance) > self.THRESHOLD_DIFF_MIN:
                        threshold = self.THRESHOLD_RELAXED
                    else:
                        threshold = self.THRESHOLD_STRICT
                else:
                    threshold = self.THRESHOLD_DEFAULT
            else:
                threshold = self.THRESHOLD_DEFAULT
            
            # Verificar se melhor match est√° dentro do threshold
            if len(sorted_alunos) > 0:
                best_aluno_id, best_distance = sorted_alunos[0]
                
                if best_distance < threshold:
                    confidence = 1 - best_distance
                    
                    if return_rankings:
                        # Preparar rankings dos top 3
                        rankings = [
                            {
                                'aluno_id': aluno_id,
                                'distance': distance,
                                'confidence': 1 - distance,
                                'num_samples': len(aluno_distances[aluno_id])
                            }
                            for aluno_id, distance in sorted_alunos[:3]
                        ]
                        return best_aluno_id, confidence, face_location, rankings
                    else:
                        return best_aluno_id, confidence, face_location
        
        return (None, 0, None, []) if return_rankings else (None, 0, None)
    
    def train_liveness_model(self, real_images, fake_images=None, epochs=10):
        """
        Treina modelo de detec√ß√£o de liveness (anti-spoofing)
        
        Args:
            real_images: Lista de imagens reais
            fake_images: Lista de imagens falsas (fotos de fotos)
            epochs: N√∫mero de √©pocas para treinamento
        
        Returns:
            bool: True se treinamento foi bem sucedido
        """
        if not CV2_AVAILABLE:
            st.warning("OpenCV n√£o est√° dispon√≠vel. Anti-spoofing desabilitado.")
            return False
        
        if not TENSORFLOW_AVAILABLE or not SKLEARN_AVAILABLE:
            st.warning("TensorFlow ou scikit-learn n√£o est√° dispon√≠vel. Anti-spoofing desabilitado.")
            return False
        
        if fake_images is None or len(fake_images) == 0:
            st.warning("Sem imagens falsas para treinar anti-spoofing. Usando detec√ß√£o b√°sica.")
            return False
        
        # Preparar dados
        X_real = []
        X_fake = []
        
        # Processar imagens reais
        for img in real_images:
            if isinstance(img, str):
                img = cv2.imread(img)
            img = cv2.resize(img, (64, 64))
            img = img / 255.0
            X_real.append(img)
        
        # Processar imagens falsas
        for img in fake_images:
            if isinstance(img, str):
                img = cv2.imread(img)
            img = cv2.resize(img, (64, 64))
            img = img / 255.0
            X_fake.append(img)
        
        # Combinar dados
        X = np.array(X_real + X_fake)
        y = np.array([1] * len(X_real) + [0] * len(X_fake))
        
        # Split train/test
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Criar modelo CNN simples
        model = Sequential([
            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),
            layers.MaxPooling2D((2, 2)),
            layers.Conv2D(64, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Conv2D(64, (3, 3), activation='relu'),
            layers.Flatten(),
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.5),
            layers.Dense(1, activation='sigmoid')
        ])
        
        model.compile(optimizer='adam',
                     loss='binary_crossentropy',
                     metrics=['accuracy'])
        
        # Early stopping
        early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
        
        # Treinar
        st.info("Treinando modelo de anti-spoofing...")
        history = model.fit(
            X_train, y_train,
            epochs=epochs,
            validation_data=(X_test, y_test),
            callbacks=[early_stop],
            verbose=0
        )
        
        # Salvar modelo
        model.save(self.liveness_model_path)
        self.liveness_model = model
        
        # Avaliar
        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
        st.success(f"‚úÖ Modelo de anti-spoofing treinado! Acur√°cia: {test_acc:.2%}")
        
        return True
    
    def detect_liveness(self, frame):
        """
        Detecta se a face √© real ou fake (foto)
        
        Args:
            frame: Frame capturado da webcam
        
        Returns:
            tuple: (is_real, confidence)
        """
        if not CV2_AVAILABLE or self.liveness_model is None:
            # Detec√ß√£o b√°sica sem modelo: sempre retorna True
            # NOTA DE SEGURAN√áA: Quando o modelo de liveness n√£o est√° dispon√≠vel,
            # o sistema permite acesso (retorna True) para manter funcionalidade b√°sica.
            # Isto significa que anti-spoofing est√° DESABILITADO neste caso.
            # A confian√ßa baixa (0.5) indica que a detec√ß√£o n√£o foi realizada.
            # O sistema ainda pode usar reconhecimento facial, mas sem prote√ß√£o contra fotos.
            return True, 0.5
        
        # Preparar frame
        img = cv2.resize(frame, (64, 64))
        img = img / 255.0
        img = np.expand_dims(img, axis=0)
        
        # Predi√ß√£o
        prediction = self.liveness_model.predict(img, verbose=0)[0][0]
        
        # prediction > 0.5 = real, < 0.5 = fake
        is_real = prediction > 0.5
        confidence = prediction if is_real else 1 - prediction
        
        return is_real, float(confidence)
    
    def mark_attendance_with_webcam(self, data_manager, timeout=30, min_confidence=0.6, confirmation_frames=3):
        """
        Marca presen√ßa usando a webcam com detec√ß√£o de face e confirma√ß√£o m√∫ltipla
        
        Args:
            data_manager: Inst√¢ncia do DataManager
            timeout: Tempo m√°ximo de espera em segundos
            min_confidence: Confian√ßa m√≠nima para reconhecimento (padr√£o: 0.6)
            confirmation_frames: N√∫mero de frames consecutivos para confirmar (padr√£o: 3)
        
        Returns:
            dict: Dados da presen√ßa registrada ou None
        """
        if not CV2_AVAILABLE:
            st.error("‚ùå OpenCV (cv2) n√£o est√° dispon√≠vel. Instale opencv-python ou opencv-python-headless.")
            return None
        
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            st.error("N√£o foi poss√≠vel acessar a webcam")
            return None
        
        st.info(f"""
        üì∏ **Sistema de Reconhecimento Inteligente**
        
        - Confian√ßa m√≠nima: {min_confidence:.0%}
        - Confirma√ß√µes necess√°rias: {confirmation_frames} frames
        - Timeout: {timeout} segundos
        
        üí° Posicione seu rosto centralizado e aguarde...
        """)
        
        placeholder = st.empty()
        metrics_placeholder = st.empty()
        stop_button = st.button("‚èπÔ∏è Parar")
        
        start_time = datetime.now()
        recognized = False
        attendance_data = None
        
        # Tracking de confirma√ß√µes
        confirmation_buffer = []
        last_aluno_id = None
        consecutive_count = 0
        
        while not recognized and not stop_button:
            ret, frame = cap.read()
            if not ret:
                break
            
            # Verificar timeout
            elapsed = (datetime.now() - start_time).total_seconds()
            if elapsed > timeout:
                st.warning("‚è±Ô∏è Tempo esgotado!")
                break
            
            # Reconhecer face com rankings
            aluno_id, confidence, face_location, rankings = self.recognize_face(
                frame, 
                return_rankings=True,
                adaptive_threshold=True
            )
            
            # Processar reconhecimento
            frame_display = frame.copy()
            status_text = "Aguardando..."
            status_color = (200, 200, 200)
            
            if aluno_id is not None and confidence > min_confidence:
                # Rastreamento de confirma√ß√µes
                if aluno_id == last_aluno_id:
                    consecutive_count += 1
                else:
                    consecutive_count = 1
                    last_aluno_id = aluno_id
                
                # Adicionar ao buffer de confirma√ß√£o
                confirmation_buffer.append({
                    'aluno_id': aluno_id,
                    'confidence': confidence,
                    'timestamp': datetime.now()
                })
                
                # Manter apenas √∫ltimos N frames
                if len(confirmation_buffer) > confirmation_frames:
                    confirmation_buffer.pop(0)
                
                # Verificar se temos confirma√ß√µes suficientes
                if consecutive_count >= confirmation_frames:
                    # Detectar liveness
                    is_real, liveness_conf = self.detect_liveness(frame)
                    
                    if is_real:
                        # Face reconhecida e confirmada!
                        recognized = True
                        
                        # Buscar dados do aluno
                        aluno = data_manager.get_record('cadastro', aluno_id)
                        
                        if aluno:
                            # Calcular confian√ßa m√©dia das confirma√ß√µes
                            avg_confidence = sum(c['confidence'] for c in confirmation_buffer) / len(confirmation_buffer)
                            
                            # Desenhar ret√¢ngulo na face
                            top, right, bottom, left = face_location
                            cv2.rectangle(frame_display, (left, top), (right, bottom), (0, 255, 0), 3)
                            cv2.putText(frame_display, f"{aluno['nome_completo']}", (left, top - 30),
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                            cv2.putText(frame_display, f"Confianca: {avg_confidence:.2%}", (left, top - 10),
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                            
                            # Registrar presen√ßa
                            now = datetime.now()
                            attendance_data = {
                                'aluno_id': aluno_id,
                                'data': now.strftime('%Y-%m-%d'),
                                'hora': now.strftime('%H:%M:%S'),
                                'tipo': 'entrada',
                                'verificado': 'Sim',
                                'confianca': f"{avg_confidence:.2%}",  # Mant√©m nome do campo para compatibilidade com banco
                                'observacoes': f"Liveness: {liveness_conf:.2%} | Confirma√ß√µes: {confirmation_frames}",
                                'data_registro': now.strftime('%Y-%m-%d %H:%M:%S')
                            }
                            
                            # Salvar no banco
                            data_manager.add_record('attendance', attendance_data)
                            
                            status_text = f"‚úÖ CONFIRMADO!"
                            status_color = (0, 255, 0)
                    else:
                        # Foto detectada!
                        top, right, bottom, left = face_location
                        cv2.rectangle(frame_display, (left, top), (right, bottom), (0, 0, 255), 3)
                        cv2.putText(frame_display, "FOTO DETECTADA!", (left, top - 10),
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                        status_text = "‚ö†Ô∏è FOTO DETECTADA!"
                        status_color = (0, 0, 255)
                        consecutive_count = 0  # Reset
                else:
                    # Ainda confirmando
                    top, right, bottom, left = face_location
                    cv2.rectangle(frame_display, (left, top), (right, bottom), (255, 165, 0), 2)
                    cv2.putText(frame_display, f"Confirmando... {consecutive_count}/{confirmation_frames}", 
                              (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 165, 0), 2)
                    status_text = f"üîÑ Confirmando... {consecutive_count}/{confirmation_frames}"
                    status_color = (255, 165, 0)
            else:
                # Reset se n√£o detectar face ou confian√ßa baixa
                consecutive_count = 0
                last_aluno_id = None
                
                if face_location is not None:
                    top, right, bottom, left = face_location
                    cv2.rectangle(frame_display, (left, top), (right, bottom), (200, 200, 200), 2)
                    cv2.putText(frame_display, f"Baixa confian√ßa: {confidence:.2%}", 
                              (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 2)
            
            # Adicionar status no frame
            cv2.putText(frame_display, status_text, (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, status_color, 2)
            
            # Mostrar frame
            frame_rgb = cv2.cvtColor(frame_display, cv2.COLOR_BGR2RGB)
            placeholder.image(frame_rgb, caption=f'Tempo: {elapsed:.1f}s / {timeout}s', 
                            use_column_width=True)
            
            # Mostrar m√©tricas em tempo real
            if rankings:
                ranking_text = "üìä **Top 3 Candidatos:**\n\n"
                for i, rank in enumerate(rankings, 1):
                    ranking_text += f"{i}. Aluno {rank['aluno_id']}: {rank['confidence']:.2%} (amostras: {rank['num_samples']})\n"
                metrics_placeholder.info(ranking_text)
            
            # Pequeno delay
            time.sleep(0.05)
        
        cap.release()
        placeholder.empty()
        metrics_placeholder.empty()
        
        # Mostrar resumo final
        if attendance_data:
            aluno = data_manager.get_record('cadastro', attendance_data['aluno_id'])
            st.success(f"""
            ‚úÖ **Presen√ßa Registrada com Sucesso!**
            
            üë§ **Aluno:** {aluno['nome_completo']}
            üìÖ **Data:** {attendance_data['data']}
            üïê **Hora:** {attendance_data['hora']}
            üìä **Confian√ßa:** {attendance_data['confianca']}
            üîí **Verifica√ß√£o:** {attendance_data['observacoes']}
            """)
            st.balloons()
        
        return attendance_data
    
    def get_student_count(self):
        """Retorna o n√∫mero de alunos registrados no sistema"""
        return len(set(self.known_face_ids))
